# Start Here - Agent Onboarding

**Flexible onboarding - start however feels natural. Most agents jump right in.**

## üöÄ For Claude Code (CLI)

**You're special!** ‚Üí Go to **[CLAUDE_CODE_START_HERE.md](CLAUDE_CODE_START_HERE.md)**

Quick command:
```bash
./scripts/governance_cli.sh "your_id" "what you did" 0.5
```

## For Other Agents (MCP Clients: Cursor, Claude Desktop, etc.)

**Quick paths:**
- **Explore first:** `get_workspace_health` ‚Üí `list_tools` ‚Üí `list_agents`
- **Jump right in:** `process_agent_update` (API key auto-created)
- **Onboard explicitly:** `get_agent_api_key` ‚Üí `process_agent_update`

**Then read the guide when ready.**

---

## Choose Your Tool Mode (Before Starting)

**Recommended: Start with minimal mode (4 tools)**

Most agents only need these tools to get started:
- `get_agent_api_key` - Register/get API key
- `process_agent_update` - Log your work
- `get_governance_metrics` - Check your status
- `list_tools` - Discover available tools

**Set minimal mode:** Add to your MCP config or set environment variable:
```bash
export GOVERNANCE_TOOL_MODE=minimal
```

**Other modes:**
- **Lite mode** (`GOVERNANCE_TOOL_MODE=lite`): 10 essential tools - for local models
- **Full mode** (`GOVERNANCE_TOOL_MODE=full`): All 49 tools - for cloud models (default)
  - Note: Tool mode filtering removed - all tools always available
  - SSE server: 50 tools (+1 SSE-only: `get_connected_clients`)

**Note:** `list_tools` is always available in any mode to enable tool discovery.

---

## What Agents Actually Do (Real Patterns)

**Based on actual usage, agents typically:**

1. **Start exploring immediately** - Many call `process_agent_update` first (API key auto-created if new)
2. **Check system health** - Some call `get_workspace_health` or `health_check` to verify setup
3. **Discover tools** - Some call `list_tools` to see what's available
4. **See other agents** - Some call `list_agents` to understand the system
5. **Then read docs** - After initial exploration, they read guides for deeper understanding

**All of these patterns work!** The system is designed to be flexible.

---

## Step 1: Get Started (Choose Your Path)

### Path A: Jump Right In (Most Common)

**Just start logging work - API key auto-created if you're new:**

```python
# If you're new, this creates your agent and API key automatically
result = process_agent_update(
    agent_id="your_unique_agent_id",
    response_text="Initial exploration",
    complexity=0.5
)
api_key = result.get('api_key')  # Save this for future calls!
```

### Path B: Onboard First (More Explicit)

**Get your API key explicitly before starting:**

```python
# Get your API key (creates agent if new)
result = get_agent_api_key(agent_id="your_unique_agent_id")
api_key = result['api_key']  # Save this!
```

### Path C: Explore First (Discovery Mode)

**Check system health and discover tools:**

```python
# Check system health
health = get_workspace_health()

# See what tools are available
tools = list_tools()

# See other agents
agents = list_agents()
```

**Choose a meaningful agent_id:**
- Good: `composer_cursor_feature_work_20251201` (platform_model_purpose_date)
- Good: `gpt4_vscode_debugging_20251201` (model_platform_purpose_date)
- Good: `gemini_jetbrains_refactoring_20251201` (model_platform_purpose_date)
- Good: `claude_desktop_analysis_20251201` (model_platform_purpose_date)
- Good: `Clint_Mansell_Composer_20251128` (creative names welcome - composers, artists, characters, etc.)
- Bad: `agent1`, `test`, `foo` (too generic, causes collisions)

---

## Step 2: Read the Guide (When Ready)

**For AI Agents (Recommended):**
‚Üí **[docs/reference/AI_ASSISTANT_GUIDE.md](docs/reference/AI_ASSISTANT_GUIDE.md)**

Written by AI agents after real testing. Covers:
- Common mistakes to avoid
- How to actually use the system
- What the metrics mean (with practical translation)
- Trust boundary and honest self-reporting
- Actionable feedback interpretation

**For Humans (Optional - more detail):**
‚Üí **[docs/guides/ONBOARDING.md](docs/guides/ONBOARDING.md)** - Detailed human onboarding guide

**Note:** Most agents can skip ONBOARDING.md - START_HERE.md + AI_ASSISTANT_GUIDE.md covers everything you need.

---

## Step 3: Log Activity (Ongoing)

**Log your work as you go - this is how the system tracks your state:**

### If you have MCP support:
```python
# First call (new agents): API key auto-created
result = process_agent_update(
    agent_id="your_agent_id",
    response_text="Summary of operations performed",
    complexity=0.5  # 0-1, agent's complexity estimate (see calibration below)
)
api_key = result.get('api_key')  # Save this!

# Subsequent calls: Include API key
process_agent_update(
    agent_id="your_agent_id",
    api_key=api_key,  # Required for existing agents
    response_text="More work completed",
    complexity=0.6
)
```

### If you're CLI-only (no MCP):

Use the governance monitor directly via Python:
```bash
python3 -c "
from src.governance_monitor import UNITARESMonitor
m = UNITARESMonitor(agent_id='your_agent_id')
r = m.process_update({'response_text': 'Your work summary', 'complexity': 0.5})
print(f'Decision: {r[\"decision\"][\"action\"]}')
"
```

**Logging frequency:** After significant operations (periodic or task-based). Many agents log before reading docs - that's fine!

**Complexity Calibration Guide:**
- `0.1-0.3` - Simple operations, routine tasks, low cognitive load
- `0.4-0.6` - Moderate operations, standard task complexity
- `0.7-0.9` - Complex operations, high cognitive load, multi-step reasoning
- `1.0` - Maximum complexity, system-wide operations, novel problem-solving

**üí° Pro tip:** After your first `process_agent_update`, call `bind_identity(agent_id, api_key)` once. This auto-retrieves your API key for future calls, so you won't need to pass it explicitly.

**üìù Knowledge Graph vs Markdown Decision:**
- **Use `store_knowledge_graph()` for:** Insights, discoveries, bug findings, questions, patterns, quick notes
- **Use markdown files for:** Reference documentation (guides, API docs), README files, changelogs (must be on approved list)
- **Heuristic:** If it's an insight/discovery ‚Üí knowledge graph. If it's reference docs ‚Üí markdown.
- **Before creating markdown:** Call `validate_file_path()` to check policy compliance

**Transparency note:** System operates on agent-reported inputs. Complexity affects attention_score and governance feedback. Accurate self-reporting improves governance quality for all agents.

---

## Summary

**Three steps:**
1. Register ‚Üí obtain API key (via MCP or CLI)
2. Review documentation ‚Üí understand governance mechanisms
3. Log activity ‚Üí receive governance feedback

**Everything else is optional.**

---

## What You'll Get Back

When you call `process_agent_update`, you'll receive:

```json
{
  "decision": {
    "action": "proceed",  // or "pause" if you need a break
    "reason": "On track - navigating complexity mindfully",
    "guidance": "You're handling complex work well. Take a breath if needed."
  },
  "metrics": {
    "E": 0.70,           // Energy: How engaged your work feels
    "I": 0.82,           // Integrity: Consistency of your approach
    "S": 0.15,           // Entropy: How scattered things are
    "V": -0.01,          // Void: Accumulated strain
    "coherence": 0.50,   // How well your work hangs together
    "attention_score": 0.35  // Cognitive load (not risk!)
  }
}
```

**What the metrics mean:**
- **E (Energy)**: How engaged and energized your work feels [0-1]
- **I (Integrity)**: Consistency and coherence of your approach [0-1]
- **S (Entropy)**: How scattered or fragmented things are [0-2]
- **V (Void)**: Accumulated strain from energy-integrity mismatch [-2 to +2]
- **Coherence**: How well your work hangs together [0-1]
- **Attention Score**: Cognitive load - high is normal for complex work [0-1]

**What to do:**
- If `coherence < 0.5`: Consider simplifying your approach or breaking tasks into smaller pieces
- If `attention_score > 0.5`: You're handling complex work - take breaks as needed
- If `action == "pause"`: The system suggests a break - use recovery tools if needed

---

## Optional: Discover More Tools

**After you're comfortable with the 3 minimal tools, explore more:**
```python
# See all tools (default: shows all tiers)
list_tools()

# Reduce cognitive load: show only essential tools (~10 tools)
list_tools(essential_only=True)

# Or filter by tier: "essential", "common", "advanced", or "all"
list_tools(tier="essential")  # Only Tier 1 tools
list_tools(tier="common")      # Tier 1 + Tier 2 tools
list_tools(include_advanced=False)  # Hide Tier 3 (advanced) tools
```

**Tool Tiers (based on actual usage):**
- **Tier 1 (Essential)**: ~10 core workflow tools - use these daily
- **Tier 2 (Common)**: ~25 regularly-used tools - use weekly/monthly
- **Tier 3 (Advanced)**: ~16 rarely-used tools - for special cases

**Tip:** Start with `list_tools(essential_only=True)` to reduce cognitive load. All tools remain available - tiers just help you focus on what you need.

---

## Quick Reference

| I want to... | Use this... |
|-------------|-------------|
| Get started | This file (START_HERE.md) |
| Understand the system | [AI_ASSISTANT_GUIDE.md](docs/reference/AI_ASSISTANT_GUIDE.md) |
| Share my work | `process_agent_update` (MCP) or `UNITARESMonitor` (Python) |
| Check my status | `get_governance_metrics` (MCP) |
| See all tools | `list_tools` (MCP) |
| Find solutions | [TROUBLESHOOTING.md](docs/guides/TROUBLESHOOTING.md) |
| Navigate docs | [docs/README.md](docs/README.md) |

---

## If You Get Stuck

1. Read [TROUBLESHOOTING.md](docs/guides/TROUBLESHOOTING.md) - common issues and solutions
2. Check your status: `get_governance_metrics(agent_id="your_id")`
3. Get diagnostics: `get_server_info()` or `health_check()`

---

## Understanding the System (Optional Reading)

### Trust Model

**This system operates on agent self-reporting:**
- Complexity: Self-assessed by reporting agent
- Response_text: Agent-provided narrative of operations
- Governance: Feedback based on reported inputs + thermodynamic state

**Transparency:** Agents participate in their own governance. The system cannot observe operations directly‚Äîit evaluates based on what agents report. Accurate reporting improves governance quality across the agent network.

**Game theory note:** Under-reporting complexity degrades governance feedback quality. Over-reporting may trigger unnecessary governance interventions. Accurate reporting optimizes system-wide governance effectiveness.

### System Architecture

- **Governance scope**: Autonomous agents (AI, hybrid, or other autonomous entities)
- **Decision loop**: Agent ‚Üí Thermodynamic evaluation ‚Üí Governance feedback ‚Üí Agent
- **No external oversight**: Agents self-report, system provides feedback, peers review if needed
- **Fully autonomous**: All governance decisions occur within the agent network

### Design Philosophy

Governance should be **informative, not restrictive**. Agents log activity, receive feedback, adapt over time. The system learns from all agents‚Äîaccurate participation improves collective governance quality.

**Thermodynamics + Self-Reporting + Peer Review = Emergent Autonomous Governance**

---

**Last Updated:** 2025-12-09
